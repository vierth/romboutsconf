{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame, Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set adjustable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert characters to clean here. Each character must be in\n",
    "# quotation marks, separated with a comma. They all should be\n",
    "# between the square brackets.\n",
    "characters_to_remove = ['』','。', '！', '，', '：', '、', '（',\n",
    "                      '）', '；', '？', '〉', '〈', '」', '「',\n",
    "                      '『', '“', '”', '!', '\"', '#', '$', '%',\n",
    "                      '&', \"'\", '(', ')', '*', '+', ',', '-',\n",
    "                      '.', '/', \"《\", \"》\", \"·\"]\n",
    "\n",
    "# Do you want to analyze common characters or a custom \n",
    "# vocabulary? \"common\" for common characters, \"custom\"\n",
    "# for custom vocabulary.\n",
    "common_custom = \"common\"\n",
    "\n",
    "# How many of the most common characters do you want to\n",
    "# include in your analysis? None will include all of them.\n",
    "# This will be ignored if you specified \"custom\" above.\n",
    "number_of_features = 100\n",
    "\n",
    "# What is the custom vocabulary you would like to analyze?\n",
    "# This will be ignored if you specified \"common\" above.\n",
    "custom_vocabulary = [\"之\", \"乎\", \"者\", \"也\"]\n",
    "\n",
    "# You can specify an n-gram range to analyze. (1, 1) will\n",
    "# just include 1-grams. (1, 3) will include 1-grams, 2-grams\n",
    "# and 3-grams. (2, 2) will include just 2-grams. (1, 1) is\n",
    "# often a good starting place.\n",
    "ngrams = (1, 1)\n",
    "\n",
    "# You can specify how many components you wish to calculate.\n",
    "# Generally, 2 is plenty.\n",
    "components = 2\n",
    "\n",
    "# Provide the name of the folder with your texts\n",
    "# I recommend calling this \"corpus\"\n",
    "corpusfolder = \"corpus\"\n",
    "\n",
    "# Specify the name of your metadata file.\n",
    "# I recommend \"metadata.txt\"\n",
    "metadatafilename = \"metadata.txt\"\n",
    "\n",
    "# Decide if you want to divide texts into equal lengths\n",
    "# True to break apart, False to not break apart\n",
    "break_apart = False\n",
    "\n",
    "# If dividing texts, how long should sections be?\n",
    "# If break_apart is False, this will be ignored\n",
    "divlimit = 10000\n",
    "\n",
    "# If you are not diving the texts, do you want to normalize\n",
    "# the results? This will calculate how often each character\n",
    "# occurs per 1,000 characters. You should ALWAYS set this to\n",
    "# True if break_apart is False. I recommend leaving this as is.\n",
    "if not break_apart:\n",
    "    normalize = True\n",
    "else:\n",
    "    normalize = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up plot parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How do you want to color the plot labels? You can color\n",
    "# them with \"genre\", \"author\", or \"era\".\n",
    "# If you have more than 15 unique categories, you may\n",
    "# encounter an error. You can add more colors to the\n",
    "# colors list in the code below, or reduce the number\n",
    "# of categories.\n",
    "plot_label_color = \"genre\"\n",
    "\n",
    "# Do you want to include text labels for each point?\n",
    "# If so, set annotate to True. Otherwise, set to False.\n",
    "annotate = False\n",
    "\n",
    "# How big would you like the plot to be? In inches.\n",
    "# (7,7) is 7 by 7 inches, (11, 8.5) is 11 inches wide\n",
    "# by 8.5 inches tall.\n",
    "plot_size = (11, 11)\n",
    "\n",
    "# If you want to save the plot to disk, please give it\n",
    "# a name. You can save it with a pdf, png, tif, svg, jpg,\n",
    "# raw, eps, ps, pgf, bmp, gif, rgba, or svgz extension.\n",
    "plot_name = \"text.pdf\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Functions (YOU DON'T NEED TO CHANGE ANYTHING AFTER THIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(instring):\n",
    "    instring = re.sub(r'~~~START\\|.+?\\|START~~~', \"\", instring)\n",
    "    instring = re.sub(r'[a-zA-Z0-9]', \"\", instring)\n",
    "    instring =  re.sub(\"\\s+\", \"\", instring)\n",
    "    unwanted_chars = characters_to_remove\n",
    "    for char in unwanted_chars:\n",
    "        instring = instring.replace(char, \"\")\n",
    "    return instring\n",
    "\n",
    "def textBreak(inputstring):\n",
    "    divlim = divlimit\n",
    "    loops = len(inputstring)//divlim\n",
    "    save = []\n",
    "    for i in range(0, loops):\n",
    "        save.append(inputstring[i * divlim: (i + 1) * divlim])\n",
    "    return save\n",
    "\n",
    "def info_for_graph(input_list):\n",
    "    unique_values = set(input_list)\n",
    "    unique_labels = [i for i in range(0, len(unique_values))]\n",
    "    unique_dictionary = dict(zip(unique_values, unique_labels))\n",
    "    class_list = []\n",
    "    for item in input_list:\n",
    "        class_list.append(unique_dictionary[item])\n",
    "    return unique_labels, np.array(class_list), unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Empty Variables For Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info_list = []\n",
    "title_list = []\n",
    "author_list = []\n",
    "era_list = []\n",
    "genre_list = []\n",
    "section_number = []\n",
    "title_author = {}\n",
    "title_era = {}\n",
    "title_genre = {}\n",
    "metadata = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This should be a tab separated file with the columns\n",
    "# filename (minus .txt), title, author, era, genre\n",
    "# Add the name of the metadata file\n",
    "metadatafile = open(metadatafilename, \"r\", encoding=\"utf8\")\n",
    "metadatastring = metadatafile.read()\n",
    "metadatafile.close()\n",
    "\n",
    "lines = metadatastring.split(\"\\n\")\n",
    "for line in lines:\n",
    "    cells = line.split(\"\\t\")\n",
    "    metadata[cells[0]] = cells[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(corpusfolder):\n",
    "    for filename in files:\n",
    "        if filename[0] != \".\":\n",
    "            f = open(root + \"/\" + filename, \"r\", encoding=\"utf8\")\n",
    "            c = f.read()\n",
    "            f.close()\n",
    "            c = clean(c)\n",
    "            \n",
    "            metainfo = metadata[filename[:-4]]\n",
    "            \n",
    "            title_author[metainfo[0]] = metainfo[1]\n",
    "            title_era[metainfo[0]] = metainfo[2]\n",
    "            title_genre[metainfo[0]] = metainfo[3]\n",
    "            \n",
    "            if not break_apart:\n",
    "                info_list.append(c)\n",
    "                title_list.append(metainfo[0])\n",
    "                author_list.append(metainfo[1])\n",
    "                era_list.append(metainfo[2])\n",
    "                genre_list.append(metainfo[3])\n",
    "\n",
    "            else:\n",
    "                broken_sections = textBreak(c)\n",
    "                \n",
    "                info_list.extend(broken_sections)\n",
    "\n",
    "                title_list.extend([metainfo[0] for i in \n",
    "                                   range(0,len(broken_sections))])\n",
    "                author_list.extend([metainfo[1] for i in \n",
    "                                    range(0,len(broken_sections))])\n",
    "                era_list.extend([metainfo[2] for i in \n",
    "                                 range(0,len(broken_sections))])\n",
    "                genre_list.extend([metainfo[3] for i in \n",
    "                                   range(0,len(broken_sections))])\n",
    "                section_number.extend([i for i in range(0, len(broken_sections))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if common_custom == \"common\":\n",
    "    vectorizer = CountVectorizer(analyzer=\"char\",ngram_range=ngrams,\n",
    "                             max_features = number_of_features)\n",
    "elif common_custom == \"custom\":\n",
    "    vectorizer = CountVectorizer(analyzer=\"char\",ngram_range=ngrams,\n",
    "                             vocabulary = custom_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count_matrix=vectorizer.fit_transform(info_list)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "if normalize:\n",
    "    dense_words = word_count_matrix.toarray()\n",
    "    corpus_dataframe = DataFrame(dense_words, columns=vocab)\n",
    "    doclengths = corpus_dataframe.sum(axis=1)\n",
    "    thousand = Series([1000 for i in range(0,len(doclengths))])\n",
    "    adjusteddoclengths = thousand.divide(doclengths)\n",
    "    per_thousand = corpus_dataframe.multiply(adjusteddoclengths, axis = 0)\n",
    "    dense_words = per_thousand.as_matrix()\n",
    "else:\n",
    "    dense_words = word_count_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the PCA object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_pca = pca.fit(dense_words).transform(dense_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and create plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    print(\"Sorry, I can't see the appropriate fonts, defaulting to Japanese\")\n",
    "    matplotlib.rc('font', family=\"TakaoPGothic\")\n",
    "elif platform == \"win32\" or platform == \"win64\":\n",
    "    matplotlib.rc('font', family=\"SimHei\")\n",
    "elif platform == \"darwin\":\n",
    "    matplotlib.rc('font', family='STHeiti')\n",
    "    \n",
    "plt.figure(figsize=plot_size)\n",
    "\n",
    "if plot_label_color == \"genre\":\n",
    "    unique_labels, info_labels, unique_genres = info_for_graph(genre_list)\n",
    "elif plot_label_color == \"author\":\n",
    "    unique_labels, info_labels, unique_genres = info_for_graph(author_list)\n",
    "elif plot_label_color == \"era\":\n",
    "    unique_labels, info_labels, unique_genres = info_for_graph(era_list)\n",
    "\n",
    "colors = [\"red\", \"blue\", \"magenta\", \"cyan\",\"black\", \"gray\", \"pink\", \n",
    "          \"orange\", \"yellow\", \"green\", \"brown\", \"beige\", \"purple\",\n",
    "          \"lavender\", \"lightblue\"]\n",
    "colors = colors[0:len(unique_labels)]\n",
    "\n",
    "# This code is partially adapted from brandonrose.org/clustering\n",
    "for color, each_class, label in zip(colors, unique_labels, unique_genres):\n",
    "    plt.scatter(my_pca[info_labels == each_class, 0],\n",
    "               my_pca[info_labels == each_class, 1],\n",
    "               label = label, color = color)\n",
    "\n",
    "\n",
    "annotate_plot = annotate\n",
    "if annotate_plot:\n",
    "    for i, text_label in enumerate(title_list):\n",
    "        plt.annotate(text_label,  xy = (my_pca[i, 0], my_pca[i, 1]),\n",
    "                     xytext=(my_pca[i, 0], my_pca[i, 1]), \n",
    "                     size=8)\n",
    "\n",
    "plt.title(\"Principal Component Analysis\")\n",
    "plt.xlabel(\"PC1: \" + \"{0:.2f}\".format(pca.explained_variance_ratio_[0] * 100)+\"%\")\n",
    "plt.ylabel(\"PC2: \" + \"{0:.2f}\".format(pca.explained_variance_ratio_[1] * 100)+\"%\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(plot_name)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=plot_size)\n",
    "\n",
    "loadings = pca.components_\n",
    "\n",
    "plt.scatter(loadings[0], loadings[1], alpha=0)\n",
    "\n",
    "plt.title(\"Principal Component Loadings\")\n",
    "plt.xlabel(\"PC1: \" + \"{0:.2f}\".format(pca.explained_variance_ratio_[0] * 100)+\"%\")\n",
    "plt.ylabel(\"PC2: \" + \"{0:.2f}\".format(pca.explained_variance_ratio_[1] * 100)+\"%\")\n",
    "\n",
    "for i, txt in enumerate(vocab):\n",
    "    plt.annotate(txt, (loadings[0, i], loadings[1, i]), horizontalalignment='center',\n",
    "                 verticalalignment='center', size=8)\n",
    "    \n",
    "plt.savefig(plot_name[:-4]+\"_loadings\"+plot_name[-4:])   \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
